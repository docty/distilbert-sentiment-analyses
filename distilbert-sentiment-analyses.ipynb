{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\n# Create a sentiment analysis pipeline\nsentiment_analyzer = pipeline(\n    \"sentiment-analysis\",\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n)\n\n# Test text\ntext = \"I absolutely love this product! Would buy again.\"\n\n# Get the sentiment\nresult = sentiment_analyzer(text)\nprint(f\"Sentiment: {result[0]['label']}\")\nprint(f\"Confidence: {result[0]['score']:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T23:25:59.104396Z","iopub.execute_input":"2025-02-14T23:25:59.104702Z","iopub.status.idle":"2025-02-14T23:26:23.589122Z","shell.execute_reply.started":"2025-02-14T23:25:59.104681Z","shell.execute_reply":"2025-02-14T23:26:23.588329Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d3a2d487ec04109b131dbed15c4056f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a59f36057d5e4068af3a1e481ef058a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6f2220de4df4650abc8199aaf44dff2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55628b8de1d847cfa63050b9d3d05abc"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Sentiment: POSITIVE\nConfidence: 0.9999\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nclass BERTSentimentAnalyzer:\n    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n        self.model.eval()\n        self.labels = ['NEGATIVE', 'POSITIVE']\n\n    def preprocess_text(self, text):\n        # Remove extra whitespace and normalize\n        text = ' '.join(text.split())\n\n        # Tokenize with BERT-specific tokens\n        inputs = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=512,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        # Move to GPU if available\n        return {k: v.to(self.device) for k, v in inputs.items()}\n\n    def predict(self, text):\n        # Prepare text for model\n        inputs = self.preprocess_text(text)\n\n        # Get model predictions\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n\n        # Convert to human-readable format\n        prediction_dict = {\n            'text': text,\n            'sentiment': self.labels[probabilities.argmax().item()],\n            'confidence': probabilities.max().item(),\n            'probabilities': {\n                label: prob.item()\n                for label, prob in zip(self.labels, probabilities[0])\n            }\n        }\n        return prediction_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T23:27:13.646163Z","iopub.execute_input":"2025-02-14T23:27:13.646767Z","iopub.status.idle":"2025-02-14T23:27:13.653848Z","shell.execute_reply.started":"2025-02-14T23:27:13.646741Z","shell.execute_reply":"2025-02-14T23:27:13.652923Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":" \n\ndef demonstrate_sentiment_analysis():\n    # Initialize analyzer\n    analyzer = BERTSentimentAnalyzer()\n\n    # Test texts\n    texts = [\n        \"This product completely transformed my workflow!\",\n        \"Terrible experience, would not recommend.\",\n        \"It's decent for the price, but nothing special.\"\n    ]\n\n    # Analyze each text\n    for text in texts:\n        result = analyzer.predict(text)\n        print(f\"\\nText: {result['text']}\")\n        print(f\"Sentiment: {result['sentiment']}\")\n        print(f\"Confidence: {result['confidence']:.4f}\")\n        print(\"Detailed probabilities:\")\n        for label, prob in result['probabilities'].items():\n            print(f\"  {label}: {prob:.4f}\")\n\n# Running demonstration\ndemonstrate_sentiment_analysis()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T23:27:41.442286Z","iopub.execute_input":"2025-02-14T23:27:41.442685Z","iopub.status.idle":"2025-02-14T23:27:41.937732Z","shell.execute_reply.started":"2025-02-14T23:27:41.442652Z","shell.execute_reply":"2025-02-14T23:27:41.936751Z"}},"outputs":[{"name":"stdout","text":"\nText: This product completely transformed my workflow!\nSentiment: POSITIVE\nConfidence: 0.9997\nDetailed probabilities:\n  NEGATIVE: 0.0003\n  POSITIVE: 0.9997\n\nText: Terrible experience, would not recommend.\nSentiment: NEGATIVE\nConfidence: 0.9934\nDetailed probabilities:\n  NEGATIVE: 0.9934\n  POSITIVE: 0.0066\n\nText: It's decent for the price, but nothing special.\nSentiment: NEGATIVE\nConfidence: 0.9897\nDetailed probabilities:\n  NEGATIVE: 0.9897\n  POSITIVE: 0.0103\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**NER**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\nclass BERTNamedEntityRecognizer:\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n        self.model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n        self.model.eval()\n\n    def recognize_entities(self, text):\n        # Tokenize input text\n        inputs = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True\n        )\n\n        # Move inputs to device\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        # print(inputs)\n\n        # Get predictions\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predictions = outputs.logits.argmax(-1)\n\n        # Convert predictions to entities\n        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n        labels = [self.model.config.id2label[p.item()] for p in predictions[0]]\n        # print(labels)\n\n        # Extract entities\n        entities = []\n        current_entity = None\n\n        for token, label in zip(tokens, labels):\n            if label.startswith('B-'):\n                if current_entity:\n                    entities.append(current_entity)\n                current_entity = {'type': label[2:], 'text': token}\n            elif label.startswith('I-') and current_entity:\n                if token.startswith('##'):\n                    current_entity['text'] += token[2:]\n                else:\n                    current_entity['text'] += ' ' + token\n            elif label == 'O':\n                if current_entity:\n                    entities.append(current_entity)\n                    current_entity = None\n\n        if current_entity:\n            entities.append(current_entity)\n\n        return entities","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T23:30:56.235972Z","iopub.execute_input":"2025-02-14T23:30:56.236361Z","iopub.status.idle":"2025-02-14T23:30:56.244042Z","shell.execute_reply.started":"2025-02-14T23:30:56.236338Z","shell.execute_reply":"2025-02-14T23:30:56.243213Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def demonstrate_ner():\n    # Initialize recognizer\n    ner = BERTNamedEntityRecognizer()\n\n    # Example text\n    text = \"\"\"\n    Apple CEO Tim Cook announced new AI features at their headquarters \n    in Cupertino, California. Microsoft and Google are also investing \n    heavily in artificial intelligence research.\n    \"\"\"\n\n    # Get entities\n    entities = ner.recognize_entities(text)\n\n    # Display results\n    print(\"Found entities:\")\n    for entity in entities:\n        print(f\"- {entity['text']} ({entity['type']})\")\n\n# Running demonstration\ndemonstrate_ner()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T23:33:13.417861Z","iopub.execute_input":"2025-02-14T23:33:13.418230Z","iopub.status.idle":"2025-02-14T23:33:13.904206Z","shell.execute_reply.started":"2025-02-14T23:33:13.418199Z","shell.execute_reply":"2025-02-14T23:33:13.903367Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Found entities:\n- Apple (ORG)\n- Tim Cook (PER)\n- AI (MISC)\n- Cupertino (LOC)\n- California (LOC)\n- Microsoft (ORG)\n- Google (ORG)\n","output_type":"stream"}],"execution_count":7}]}